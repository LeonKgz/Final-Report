\chapter{LoRa}

LoRa stands for \textbf{Lo}ng \textbf{Ra}nge. It is a transmission standard that aims at achieving long range reach with low power
consumption, most commonly used by the Internet of Things devices.
LoRa technology was first developed by Cycleo from Grenoble, France around 2010 with first prototypes implementing point-to-point communication for walkie-talkie and metering applications \cite{trinity_panel}. Later in 2012 the Cycleo team was aquired by Semtech, and released the chips for LoRa end devices and gateways \cite{origins}. [HERE TALK ABOUT LORAMAC AND PHYSICAL LAYER]\\

The open non profit organisation LoRa Alliance emerged in 2015\cite{alliance} [MB NOT REFERENCE JUST A WEBSITE]. 
It is an association of companies with Semtech being one of
its founding members \cite{alliance_founder} backing up the LoRaWAN protocol, insuring its
global presence and acceptance. The total number of members of the alliance has surpassed 500 in 2017 \cite{500_members}. Among the notable members are Cisco, Amazon and Alibaba \cite{alliance_members}.\\

\section{Basics}

Before diving into the specifics of LoRa and how it operates, let's have a look at the fundamental technologies and techniques behind it. \\

The most well known ways of transmitting information 
via electro-magnetical waves are AM and FM — amplitude and frequency modulation. In addition to radio broadcasting both these techniques have a range of applications such as radar, EEG, computer modems. When 
on is concerned with digital communications we want to 
encode ones and zeroes into the waveforms of the EM waves. For AM it is possible. \\

One of the main specifications about the proprietary standard LoRa is the one published by Semtech itself \cite{semtech_spec}. In the next few sections I will try to expand on the selected parts of it and explain them in detail.

\section{LoRa Physical layer}

\subsection{Shannon–Hartley theorem}

As the spec recognizes, an important starting point is the Snannon-Hartley theorem or also referred to as The Shannon limit - a significant milestone in information theory \cite{mit_article_on_shannon} . What it basically describes is the maximum data rate that a channel can have transmitted error-free in a particular bandwidth subject to noise. Here is the formula referenced by Semtech \cite{semtech_spec}, first introduced in the original paper \cite{shannon} by Claude Shannon:
\begin{align}
    C = B \times log_2 (\frac{S}{N} + 1) \label{eq:shannon}
\end{align}
where:
\begin{align*}
    C &- \text{channel capacity (in bits/s)}, \\
    B &- \text{channel bandwidth (in Hz)},\\
    S &- \text{average received signal power (in Watts)},\\
    N &- \text{average noise or interference power (in Watts)},\\ 
\end{align*}
Following the approximation from LoRa specification \cite{semtech_spec} assuming such noise that $\frac{S}{N} <<$ 1 applying the approximation to the algorithm:
\begin{align}
    log_2(1 + \frac{S}{N}) &= \frac{ln(1+ \frac{S}{N})}{ln2} \approx \frac{1}{ln2}\times \frac{S}{N} \approx 1.443 \times \frac{S}{N} \approx \frac{S}{N} \\
\end{align}
Subsequently we get:
\begin{align}
    \frac{C}{B}& \approx \frac{S}{N} \\
\end{align}
Which is equivalent to:
\begin{align}
    \frac{N}{S} & \approx \frac{B}{C}
\end{align}



As can be seen from the formula, to transmit error free data with noise-to-signal ratio fixed, one only needs to increase the signal bandwidth \cite{semtech_spec}.

\subsection{Direct Sequence Spread Spectrum (DSSS)}

Direct Sequence Spread Spectrum is a signal transmission technique based on increasing the bandwidth of the original data generated by the sender by multiplying it by the spreading code — a sequence of bits, also known as chips \cite{dsss_article}. [POSSIBLY INCLUDE DIAGRAMS TO DEMOSTRATE ECNODING AND DECODING] The signal is decoded at the receiver by again multiplying the signal with
the same spreading sequence of bits \cite{dsss_article}. One of the disadvantages of this transmission technique is that it is too power hungry and demanding for low-power devices or networks with small resources \cite{semtech_spec}.

\subsection{LoRa Spread Spectrum (Chirp Spread Spectrum)}

CSS is a modulation technique that has been around since 1940s \cite{semtech_spec} with 
applications in naval and air military and also observed in fauna \cite{origins}. What makes CSS different from other Spread Spectrum techniques (despite some claiming it to be a subtype of DSSS \cite{orthogonality_description}) is the fact that it is resistant to multi-path fading and Doppler effect while having relatively moderate transmission power demands \cite{semtech_spec}. \\

According to IEEE standard for information technology \cite{ieee_2007}: \\

\begin{addmargin}[4em]{2em}% 1em left, 2em right
«A chirp is a linear frequency modulated pulse. It could be thought of as sweeping the band at a very high speed. The type of CSS system defined for this standard uses patterns of smaller chirps, or ’sub chirps’, to build one larger chirp symbol.» \\
\end{addmargin}

When describing CSS symbols we are mainly concerned with 4 parameters. 
Spreading factor ($SF$), minimal frequency $f_{min}$, maximal frequency $f_{max}$ and the input bits. Generation of a CSS symbol starts at the starting frequency $f_0$ lying between $f_{min}$ and $f_{max}$ as can be seen on the figure \ref{fig:slope}. The difference between $f_{min}$ and $f_{max}$ (i.e. $f_{max} - f_{min}$) is called bandwidth — B. $f_0$ represents the input information of $SF$ bits. Thus the symbol duration can be calculated using this formula from Application Note Semtech AN1200.22 \cite{semtech_spec}: 
\begin{align}
    T_S = \frac{2^{SF}}{B} 
\end{align}

% Spreading factor determines the number of bits to be encoded in a signal \cite{sf}

% LoRa employs six spreading factors from 7 to 12 to provide orthogonality in data transmission (collision-free) occurring on the same frequency.  SF determines the chirp rate — change in frequency with respect to time which is the slope on \ref{fig:slope}{}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.5]{figures/slope.PNG}
  \caption{Graphical representation of the frequency in time for
two CSS symbols with 2 different spreading factors. Source: \cite{slope_diagram}}
  \label{fig:slope}
\end{figure}

% An advantage of LoRa spread spectrum is that timing and frequency offsets between transmitter and receiver are equivalent, greatly reducing the complexity of the receiver design. \cite{semtech_spec}  

\subsection{Spreading factors}

According to the spec \cite{semtech_spec}
LoRa employs 6 spreading factors from 7 to 12. Spreading factor value is the number of bits a symbol can represent \cite{sf_article} and looking at figure \ref{fig:slope} it determines the rate of the frequency change in a sweep. 

Spreading factors allow different signals to be transmitted
over the same channel at the same time. As the specification claims it is possible due to the orthogonality of CSS signals of different spreading factors to each other \cite{semtech_spec}.
Despite the claim of the orthogonality feature of spreading factors in CSS signals, it has been shown to be quasi-orthogonal and the decoding of signal at the receiver
can be disrupted under certain conditions for example if the power of the interfering signal is stronger than the one to decode \cite{imperfect_1}.

\section{LoRaWAN}

On top of the physical layer there needs to be a protocol.
And such is the LoRaWAN layer backed by the LoRa Alliance. 
As the LoRa Alliance itself states on its website 
\cite{lora_alliance_about_lorawan} LoRaWAN is 
"a networking protocol designed to wirelessly connect battery operated 
‘things’ to the internet in regional, national or global networks".

\subsection{Network Topology}

The LoRaWAN standard is based on a star topology.
Multiple end devices (nodes) communicate with a gateway (base station) and multiple gateways communicate with the main network server as demonstrated in the figure \ref{fig:star}. Here gateway acts as an agent between the nodes and the internet, translating Radio Frequency messages into Internet Protocol packets and vice-versa \cite{lora_alliance_about_lorawan}.
One gateway can typically cover a range of hunderds of meters to tens of kilometers with up to millions of end devices \cite{doppler}.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.4]{figures/start-topology.PNG}
  \caption{LoRaWAN nodes and gateways connected to a LoRa server Source: \cite{doppler}}
  \label{fig:star}
\end{figure}

\subsection{Multiple Access Control (MAC) scheme}

LoRaWAN implements ALOHA protocol where any device can wake up and transmit a message regardless of time. We are concerned with the messages between a LoRaWAN device and a gateway (base station). Requests from a node to a gateway are called uplink messages, and the responses are called downlink messages.  
The communication scheme has an uplink-centric design and 
LoRaWAN mandates this design for all devices implementing the protocol \cite{simulator}. In the figure \ref{fig:class A} one can see the typical cycle a node goes through while communicating with a particular gateway. The receive windows are allocated periods of time when the node is expecting 
a downlink response that should follow its uplink request.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.5]{figures/class A.PNG}
  \caption{Class A type LoRaWAN device scheduling of receive slots for downlink messages. Source: \cite{doppler}}
  \label{fig:class A}
\end{figure}

There are 3 different device types based on the way to schedule the receive window slots for downlink communication: class A, B and C. Support for class A (demonstrated in the figure \ref{fig:class A} above) is compulsory with B and C being extensions to it. Class A devices schedule 2 receive windows following an uplink transmission, class B devices open receive windows periodically and class C devices listen for downlink messages continuously unless they are transmitting \cite{lora_alliance_spec}. 

\subsection{Duty cycle}
For each band LoRaWAN devices operate in, they need to comply with the industrial, scientific and medical (ISM) regulations often set by the local government \cite{duty_cycle}. These include the duty cycle limitation that determines amount of time a device can be active within a bandwidth. For example in Europe the duty cycle standards are set by the 
European Telecommunications Standards Institute (ETSI) —  "the recognized regional standards body dealing with telecommunications, broadcasting and other electronic communications networks and services.", as their website  \cite{about_etsi} states.\\ 

What duty cycle basically controls is the fraction of time a device can be active on a particular sub-band (potentially on multiple channels) \cite{duty_cycle}. For example if the channel in 
question is used for 1 time unit for every 10 time units,
the device using it has a duty cycle of 10\%. If we take into account multiple channels (within one sub-band), then we count in the total fraction of time these channels are being used by the device \cite{duty_cycle}. [INFO ABOUT section 7.2.3 of the ETSI EN300.220 standard]

\section{NOMA}
 
 Non-Orhogonal Multiple Access (NOMA) scheme is a relatively new concept first introduced in this publication \cite{noma_original} in 2013. As opposed to orthogonal
frequency division multiplexing (OFDM), NOMA decodes multiple incoming signals, occupying the same time 
or frequency resources, utilizing the power or code domain \cite{noma_imperial}. [DIAGRAM FROM TUTORIAL]. In the power domain implementation of NOMA the decoding is achieved via successive interference cancellation (SIC) based on the 
different power levels of the incoming signals \cite{noma_imperial} \cite{noma_original} ??? .\\ 

Despite the original paper \cite{noma_original} focusing on the downlink NOMA
implementation (i.e. end-device/node being the receiver in question), we are interested in the uplink communications and thus the workings of NOMA at the gateway. 
  
%  As the name NOMA suggests, it it does not utilizes the orthogonality of 
%  , but instead decodes multiple incoming signals based on the 
 
 
\chapter{Simulation Framework}

In order to simulate a LoRaWAN network I have decided to pick one described in this paper \cite{simulator}. The developers of the paper have closely followed guidelines from The Things Network (TTN)\footnote{https://www.thethingsnetwork.org/}, which is an open community of volunteers that took on an initiative to contruct and maintain LoRaWAN stations around the world to reach its global coverage. On their website they also provide tutorials and guides to help people deploy LoRaWAN gateways and nodes. The simulation code is published in github \cite{simulator_github}. The developers have decided to simulate the LoRaWAN network via the \texttt{SimPy} which is a Python discrete-event simulation framework library\cite{simpy}. The simulation is mainly driven through continuous yielding of appropriate generator functions. A potential improvement to the code would be introduction of some sort of parallelisation.

\subsection{Architecture}

The simulation is based on the LoRaWAN architecture of star-toppology network with mutiple nodes connected to a single gateway as already described above and in the figure \ref{fig:star}. The simulation supports only one gateway, so among other improvements would be introduction of multiple gateways in the simulated area. \\

In the following figure \ref{fig:architecture} is the scheme describing relations between the most relevant simulation python classes (along with their names).

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.5]{figures/architecture.PNG}
  \caption{Interaction between python classes: \cite{simulator}}
  \label{fig:architecture}
\end{figure}

\subsection{Nodes}

As can be seen from the figure \ref{fig:architecture} the \texttt{Node} object is characterized by the \texttt{Location}, \texttt{Energy Profile} and \texttt{LoRa Parameters} objects (among others) passed to its constructor during initialization as parameters.\\

\texttt{Location} as its name suggests, simply holds the coordinates for a particular Node. The simulation assumes a rectangular cell, and the particular node location is typically randomly selected within the bounds of that rectangle. \\

\texttt{Energy Profile} holds information about the different power states and their duration in the execution loop of communication between a node and its respective gateway (sketched above in \ref{fig:class A}). The energy profile used in the original paper \cite{simulator}, as the authors stated was motivated by the energy consumption analysis of another project — LoRaWAN implementation for the EFM32 Happy Gecko develop board \cite{energy_profile}. The energy profile used is displayed in the figure below:

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{figures/energy_profile.PNG}
  \caption{Energy Profile devised from \cite{energy_profile}. Source: \cite{simulator}}
  \label{fig:energy_profile}
\end{figure}\\

The effect of implementing this energy profile can be seen in the figure \ref{fig:power_states} below; it displays, how the power states of a node develop in time:

\begin{figure}[h!]
  \centering
  \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/class A 2.PNG}
  \caption{Power states of a LoRa node simulated in \cite{simulator}}
  \label{fig:power_states}
\end{figure}\\

\texttt{LoRa Parameters} object holds information about the configuration of the uplink packets that the Node is going to send, such as its spreading factor, frequency (channel) and transmission power. These are the factors that play a crucial role in the resulting energy consumption at the node [CITE THAT???] .

\subsection{AirInterface}

\texttt{AirInterface} is the object via which \texttt{Node} and \texttt{Gateway} communicate. It processes the packets "in the air" and determines their airtime as well as simulating collisions between packets. 2 important objects that define it are \texttt{Propagation Model} and \texttt{SINR Model}.\\

\texttt{Propagation Model} calculates the path loss 
of a signal traveling in the channel through the air.
the formula is defined in the original paper \cite{simulator} :
\begin{align}
    PL(d) = PL(d_0) + 10n\times log\frac{d}{d_0} + X_{\sigma} [dB] \label{eq:propagation_model}
\end{align}

Where the following parameters are used (parameters are taken for granted by the authors \cite{simulator} from another source \cite{propagation_model_parameters} [MORE DETAIL ON WHY AND HOW]):
\begin{align*}
     d_0 & = 1000 \text{m}\\
     PL(d_0) & = 128.95 \text{dB}\\
     X_{\sigma} & = 7.8 \text{dB}\\
     n & = 2.32\\
\end{align*}

The \texttt{SINR Model} is implemented similarly to how \texttt{SNR Model} was implemented in the original simulator. The model calculates signal-to-interference noise ratio (SINR) for the received signal strength values of packets in the air according to this formula [CITE WIKIPEDIA???]:
\begin{align}
    \text{SINR}(x) = \frac{P}{I + N} \label{eq:sinr}
\end{align}
where:
\begin{align*}
     P &- \text{power of the examined signal}\\
     I &- \text{total interference from weaker signals in the air}\\
     N &- \text{ noise floor}\\
\end{align*}

The interference is only calculated from the weaker signals, since we're assuming that NOMA is enabled at 
the gateway and thus all the stronger signals would be
decoded apriori \cite{noma_original}.\\

\texttt{SINR Model} also calculates throughput from SINR values. The need for throughput metric will be explained below. This formula [CITE WIKI???] is used:

\begin{align}
    \text{throughput} = \text{$log_{2}$}(\text{sinr} + 1)
\end{align}

Collision model is not implemented as a separate object but through a collection of functions in \texttt{Air Interface}. The model implementation is motivated by the authors \cite{simulator} by the findings from another paper \cite{collision_conditions}, that looks at how reception overlap, carrier frequency, spreading factor and power can create conditions for a packet collision. The authors \cite{simulator} also assume that signals with different spreading factors are practically orthogonal and can be demodulated without colliding.

\subsection{Gateway}

\texttt{Gateway} object is responsible for receiving 
the uplink packets from the \texttt{Air Interface}. In order for a packet to come through it must not collide and have  a signal strength above that of gateway's specified sensitivity. In the original simulator \cite{simulator} \texttt{Gateway} is also responsible for executing the ADR algorithm. ADR is the algorithm, devised to minmize the energy consumption at nodes, operating mainly by estimating the appropriate TP and SF values for each uplink packet and sending them to the respective node (that sent the packet) via a downlink frame \cite{simulator}. The node that uses these values until a next update from the gateway. In the original simulator these values are calculated based on the SNR values of the received frames. 
[INPUT DETAILS ON ADR and ADR itself maybe] \\

In this project, however we are not going to implement any form of ADR and instead apply reinforcement learning tecnhiques to 
decide on appropriate \texttt{LoRa Parameters} values for uplink frames.


% Due to the SF orthogonality assumption, the gateway can simultaneously receive up to 8 different packets on separate channels. [FALSE???]

\chapter{Reinforcement Learning} 

Reinforcement Learning has recently seen a rise in popularity after a few successful case studies such as the success of DeepMind in 2016 after AlphaGo beat the Korean 9-dan professional Go player \cite{alpha_go_lee_sedol} and after more recently OpenAI beat a team of professionals in a Dota 2 match \cite{dota}.\\

Reinforcement Learning (RL) typically focuses on the problems of agents within some environment. The agents try to learn optimal behaviour within the environment, with the goal of maximizing some sort of reward (e.g. AlphaGo playing Go, trying to maximize it's win probability \cite{alpha_go_lee_sedol}). The policy of that agent, i.e. how it decides to act under certain conditions (e.g. coordinates on a map, temperature, board game position) is the component, not specified by the programmer but instead learnt by the agent via trial-and-error interactions with the environment over the learning period \cite{sutton_barto}. Thus RL often reduces to a problem of learning the optimal policy in the context of a particular application.\\

This closed loop can be demonstrated as in the figure \ref{fig:closed_loop} below, where an agent subject to state $S_{t}$
and reward $R_{t}$ takes an action $A_{t}$ which leads to a new
state $S_{t+1}$ and a new reward $R_{t+1}$:

\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.7]{figures/closed_loop.PNG}
  \caption{Closed-loop of reinforcement learning. Source: \cite{sutton_barto}}
  \label{fig:closed_loop}
\end{figure}\\  

% \section{Terminology and symbols}
% Here I'll explain the main bulk of terminology specific to reinforcement learning that we'll need for the rest of this chapter.\\

% Rewards, Action, States, Policy, Value function [DO LATER]
 
\section{Markov Chain}

Markov Chain is essentially a way to model stochastic
processes with
particular properties. As the name suggests it is a chain of
states that describes the process we are modeling. Using an
exmaple from this article \cite{markov_chain_article}, if we
are to model weather at a particular region on a daily basis,
we could have 2 states: Sunny and Rainy. Now what interests us
are the transitions between these 2 states. We could model
them as a simple random process such that the probability of
it being Sunny after being Rainy was 0.5 and the probability
of it being Rainy after being Sunny was 0.1. Furthermore if we
assume the \textbf{memorylessness} property of this model i.e.
that the weather tomorrow is \textbf{independent} of the
weather yesterday and only depends on the weather today then
we could effectively derive the other 2 missing probabilites
for the weather staying Sunny (1 - 0.1 = 0.9) or Rainy (1 -
0.5 = 0.5) for 2 days in a row \cite{markov_chain_article}. This example is demonstrated in figure \ref{fig:weather}.

\begin{figure}[H]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.7]{figures/markov_weather.PNG}
  \caption{Markov chain for weather forecasting. Source: \cite{markov_chain_article}}
  \label{fig:weather}
\end{figure}

Thus we can describe a Markov process through a set of states S and a probability transition matrix P satisfying the \textbf{Markov property} (memorylessness) \cite{markov_chain_article}:
\begin{align}
    P_{ss'} = P[S_{t+1} = s' | S_{t} = s]\\
    \sum_{s' \in S} P_{ss'} = 1
\end{align}

\section{Markov Reward Process}
Markov Reward Process is a Markov Process that 
has a reward value tied to each state transition \cite{lecture_lets_go_markov}. We can describe these rewards through another vector R such that:
\begin{align}
    R_{s} = E[r_{t+1} | S_{t} = s] \label{eq:return_matrix}
\end{align}

This is the reward that the agent leaving state s receives upon transitioning at timestep $t+1$.\\

The process is also defined by a discount factor $\gamma \in [0, 1]$ \cite{lecture_lets_go_markov}. We need it to define the discounted total return value $R_t$ which is the estimated sum of all immediate rewards $r$ starting from time-step t:
\begin{align}
    R_t = r_{t+1} + \gamma r_{t+2} + \gamma ^2 r_{t+3} + ... = \sum^{\infty}_{k=0}\gamma^{k}r_{t+k+1} \label{eq:return}
\end{align}

The discount factor allows the agent to put different weight on immediate and distant rewards,
with smaller $\gamma$ being more concerned with immediate results and larger $\gamma$ looking far into the future \cite{lecture_lets_go_markov}. 

% [WHY DISCOUNTING IS A GOOD IDEA SLIDE 56 https://materials.doc.ic.ac.uk/view/2021/70028/Course\%20Material/7]
\subsection{State Value Function}
State Value Function $v$ is the estimate of the total return one expects to retrieve starting from a state $s$ at time $t$ in the context 
of a Markov Reward Process \cite{lecture_lets_go_markov}.
\begin{align}
    v(s) = \EX[R_t | S_t = s] \label{eq:value_function_def}
\end{align}

If we expand the $R_t$ term as in \ref{eq:return} and apply the Law of Total expectation along with definition from \ref{eq:return_matrix} we get the following derivation \cite{lecture_lets_go_markov}:
\begin{align}
    v(s) &= \EX[R_t | S_t = s] \label{eq:value_function}\\
         &= \EX[r_{t+1} + \gamma r_{t+2} + \gamma ^2 r_{t+3} + ... | S_t = s] & \text{definition \ref{eq:return}} \nonumber \\
         &= \EX[r_{t+1} + \gamma (r_{t+2} + \gamma r_{t+3} + ...) | S_t = s] \nonumber\\
         &= \EX[r_{t+1} + \gamma R_{t+1} | S_t = s] & \text{definition \ref{eq:return}}  \nonumber\\
         &= \EX[r_{t+1} + \gamma \EX[R_{t+1} | S_t = s] | S_t = s] & \text{Law of Total expectation}  \nonumber\\
         &= \EX[r_{t+1} + \gamma v(S_{t+1})| S_t = s] & \text{definition \ref{eq:value_function_def}}  \nonumber\\
         &= R_s + \gamma \sum_{s' \in S} P_{ss'}v(s') & \text{expanding $\EX$ term + definition  \ref{eq:return_matrix}} \nonumber
\end{align}

Converting the last line of the above derivation into vector notation to account for n states (respectively n dimensions):
\begin{align}
    \textbf{v} = R + \gamma P \textbf{v}
\end{align}

A direct solution to this would be 
\begin{align}
    \textbf{v} = (\bbone - \gamma P)^{-1}R
\end{align}

Due to matrix inversion being an expensive computational operation, this kind of approach of estimating a state value function is not preferred and only works for relatively small MRPs \cite{lecture_lets_go_markov}. There are, however iterative approaches to this problem, such as Dynamic Programming, Monte-Carlo evaluation and Temporal Difference Learning.

\section{Policy}
As mentioned above an agent within an environment is capable of taking an action a. The sequence of actions taken under 
various conditions is defined by the agent's policy.
A policy $\pi$ is typically a conditional probability distribution
that allows an agent to choose its next action in a defined state at a defned time \cite{lecture_lets_go_markov}:
\begin{align}
    \pi_t(a, s) = P[A_t = a | S_t = s]
\end{align}

\section{Markov Decision Process}

Combining Markov Reward Process with a policy yields Markov Decision Process, defined by a state space S; action space A, transition probablity matrix $P^{a}_{ss'} = p(s_{t+1}| s_t, a_t)$, discount factor $\gamma \in [0, 1]$, reward function $R^{a}_{ss'}$ and policy $\pi$ \cite{lecture_mdp}. 

\subsection{Value Function}
If we now account for an MDP policy $\pi$ and expand the original definition \ref{eq:value_function_def} \cite{lecture_mdp}:
\begin{align}
    V^{\pi}(s) &= \EX_{\pi}[R_{t} | S_t = s]  \nonumber \\
    &= \EX[\sum^{\infty}_{k=0} \gamma^k r_{t+k+1} | S_t = s] \nonumber \\
    &= \EX[r_{t+1} + \gamma \sum^{\infty}_{k=0} \gamma^k r_{t+k+2} | S_t = s] \nonumber \\
    &= \sum_{a \in A} \pi(s, a) \sum_{s' \in S} P^{a}_{ss'} (R^{a}_{ss'} + \gamma V^{\pi}(s')) &\text{expanding $\EX$} \label{eq:mdp_value_function_expansion}
\end{align}

where 
\begin{align*}
    \pi(a, s) &= P[a|s] &\text{policy i.e. probability of taking action $a$}\\
    P^{a}_{ss'} &= P[s' | s, a] &\text{state-action transition probability}\\
    R^{a}_{ss'} &= r(s, a, s') &\text{immediate reward function}
\end{align*}

Majority of all the reinforcement learning algorithms introduced in this chapter later down below will rely on this "recursive consistency" \cite{lecture_mdp} property of state value functions. 
\subsection{Policy Evaluation}
Now we need to find a systematic way of estimating the value
function for a given MDP for an arbitrary policy. This process is called Policy Evaluation \cite{lecture_mdp}. The most obvious method is to apply the Bellman equation over and over again, until the value function converges as in figure \ref{fig:ipa}.
\begin{figure}[H]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.8]{figures/iterative_policy_evaluation.PNG}
  \caption{Iterative Policy Algorithm; Source: \cite{lecture_mdp}}
  \label{fig:ipa}
\end{figure}
\subsection{State Action Value Function}
As an extension to the state value function we introduce the state-action value function $Q$ that evaluates the expected return for taking action $a$ at a state $s$ subject to policy $\pi$ \cite{lecture_mdp} :
\begin{align}
    Q^{\pi}(s, a) = \EX[R_t | S_t = s, A_t = a] = \EX[\sum^{\infty}_{k=0} \gamma^k r_{t+k+1} | S_t = s, A_t = a]
\end{align}

The relationship between state value and state-action value functions is as follows:
\begin{align}
    V^{\pi}(s) = \sum_{a \in A} \pi(s, a) Q^{\pi}(s, a) 
\end{align}

\subsection{Optimal Value and Optimality equations}
If we are to optimize the state value function V we are to determine the optimal ploicy. The Optimal Value Function $V^{*}$ is \cite{lecture_mdp}:
\begin{align}
    V^*(s) = \max_{\pi} V^{\pi}(s)   \forall s \in S
\end{align}

Following similar derivation as in \ref{eq:mdp_value_function_expansion}:
\begin{align}
    V^*(s) = \max_{a}\sum_{s'} P^{a}_{ss'} (R^{a}_{ss'} + \gamma V^{*}(s'))
\end{align}

Respectively for the optimal state-action value function $Q^{*}$:
\begin{align}
    Q^*(s, a) = \sum_{s'} P^{a}_{ss'} (R^{a}_{ss'} + \gamma \max_{a'}Q^{*}(s', a'))
\end{align}

Applying the Bellman Optimality Equation is one way to solve the problem of finding the optimal policy. But this relies on 3 assumptions: the environment model is well-known, we have computational resources to brute force the solution and of course the Markov property \cite{lecture_mdp}. So as often happens approximate solutions are searched.

\subsection{Optimality Convergence Theorem}
% "Theorem (Bellman Optimality Equation convergence theorem):\\
% For an MDP with a finite state and action space:\\
%  - The Bellman Optimality Equations have a unique solution and the values produces by the value iteration converge to the solution of  the Bellman equations"\\
%  This can be proved through a Contraction Mapping Theorem.

\section{Dynamic Programming}
Dynamic Programming typically refers to a set of algorithms targeted at solving problems within well-defined systems such as Markov Decision Process \cite{lecture_dp}. Typically Dynamic Programming refers to solving a problem by breaking it into smaller sub-problems and solving them recursively.

\subsection{Policy Improvement Theorem}
According to \cite{lecture_dp} the theorem is as follows: given policies $\pi$ and $\pi'$ such that $\forall s \in S$
\begin{align}
    Q^{\pi}(s, \pi'(s)) \geq V^{\pi}(s).
\end{align}

Implies that $\pi'$ is at least as good as $\pi$:
\begin{align}
    V^{\pi'}(s) \geq V^{\pi}(s), \forall s \in S
\end{align}

The theorem can be proved in a series of expansions \cite{lecture_dp}.

\subsection{Policy Iteration}
Policy Improvement Theorem allows us to build an improvement mechanism for policies. It is possible to build a loop that
continuously updates the policy to an optimum. In this way we can find the optimal state value function. The process in which we combine Policy Evaluation and Policy Improvement is called Policy Iteration \cite{lecture_dp}.

% \subsection{Bellman's Principle of Optimality}
% As the lectures states Bellman's Principle of Optimality:\\

% A policy $\pi(a, s)$ achieves the optimal value from state $s$, \vpidash{s} = $V^{*}(s)$, if and only if:\\ 
% 1) For any state $s'$ reachable from s, i.e. $\exists a : p(s', s, a) > 0$\\
% 2) $\pi$ achieves the optimal value from state s', \vpi{s'} = \voptimal{s'}
 
% \subsection{Policy Iteration Algorithms}

\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.6]{figures/policy_iteration_algorithm.PNG}
  \caption{Policy Iteration Algorithm; Source: \cite{lecture_dp}}
  \label{fig:pia}
\end{figure}

\subsection{Value Iteration Algorithm}

There are multiple ways to define the Policy Evaluation loop stop conditions: $\epsilon$-convergence of value function or stopping after k iterations, or even just after one sweep of state space $S$ turning Bellman Optimality Equation into an update rule, thus obtaining the Value Iteration Algorithm \cite{lecture_dp}. 
\begin{figure}[H]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/value_iteration_algorithm.PNG}
  \caption{Value Iteration Algorithm; Source: \cite{lecture_dp}}
  \label{fig:via}
\end{figure}

\subsection{Bootstrapping}

DP works well for moderate not resource hungry type of problems and suffers from the curse of dimensionality once the complexity increases. It also requires full knowledge of the Markov Decision Process which is not always available in practice. Despite that it introduces a useful concept: "bootstrapping" i.e. "updating values estimates based on other value estimates" \cite{lecture_dp}. To address the weaknesses
of dynamic programming Model-Free Learning comes into play.

\section{Model-Free Learning}
Model-Free learning as its name suggests focuses on way
of solving a problem without the need of full model knowledge,
like in dynamic programming. 
\subsection{Monte Carlo (MC) methods}

MC methods learn directly from real experience. No model knowledge is required because learning is conducted on finished traces of episodes that end in a terminal state (and respectively bootstrapping is not needed as well) \cite{lecture_mfl} . The basic idea is to calculate the average return per episode.

\subsection{MC Policy evaluation}
If we're to implement Policy Evaluation in the context of Monte-Carlo:
\begin{figure}[H]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/mc_policy_evaluation.PNG}
  \caption{MC Policy Evaluation Algorithm; Source: \cite{lecture_mfl}}
  \label{fig:via}
\end{figure}

Here you can see that value function is approximated with an average returns of states. This is a First-Visit MC algorithm, so only the first occurence of a state is counted toward the estimate. In Every-visit MC algorithm every occurence is recorded \cite{lecture_mfl}.

We can update the policy after every episode or after
sampling a trace of 20 for example. the former is called online MC and the latter Batch MC.

\subsection{Incremental Monte Carlo updates}
We can devise an incremental update from the normal definition of 'mean' as follows \cite{lecture_mfl} :
\begin{align}
    \mu_k = \mu_{k - 1} + \frac{1}{k} (x_k - \mu_{k - 1})
\end{align}

This is an important milestone, since we can keep the running mean without storing traces of episodes. \\

In the same way we can update the state value function $V(s)$ incrementally after each new episode (s, a, r, s'):
\begin{align}
    V(s_t) \longleftarrow V(s_t) + \frac{1}{N(s_t)}(R_t - V(s_t))
\end{align}

Moreover one can set the $N(s_t)$ value (i.e. current number of episodes) to another number $n$, such that $n < N(s_t)$, by basically narrowing down the averaging operation to the last n episodes. This is in effect turns it into $\alpha$ -- the rate of "forgetting old episodes" \cite{lecture_mfl}:
\begin{align}
    V(s_t) \longleftarrow V(s_t) + \alpha (R_t - V(s_t)) \label{eq:mc_increment}
\end{align}

\subsection{Temporal Difference}
We have now introduced two different approaches to finding the optimal Value function for a problem at hand: Dynamic Programming and Monte-Carlo. Dynamic programming achieves the optimal solution through "bootstrapping" i.e. estimating state values through
neighbouring states. Monte-Carlo methods calculate the estimates through sampling directly from experience i.e. traces of episodes. \\

Temporal Difference (TD) is what combines DP and MC: like MC It does not require knowledge of Markov Decision Process at hand and 
learns from episodes of experience and like DP it makes use of bootstrapping \cite{lecture_mfl}.

\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/TD_diagrams.PNG}
  \caption{Temporal Difference; Source: \cite{lecture_mfl}}
  \label{fig:td}
\end{figure}

\subsection{Update rule}
The combination of DP and MC is manifested in the value function update rule for Temporal Difference:
\begin{align}
  V(s_t) \longleftarrow V(s_t) + \alpha (r_{t+1} + \gamma V(s_{t+1}) - V(s_t)) \label{eq:td_update}
\end{align}

where:
\begin{align*}
  r_{t+1} + \gamma V(s_{t+1}) - V(s_t) &\text{ — } \text{TD error}\\
  r_{t+1} + \gamma V(s_{t+1}) &\text { — } \text{TD target}
\end{align*}

Comparing this to the update rule for MC (equation \ref{eq:mc_increment}) we can see that in addition to the actual measured reward $r_{t+1} (R_{t}$ in equation \ref{eq:mc_increment}) we add an estimate of the next state i.e. $V(s_{t+1})$. This estimation refers us back to the original estimation of the value function from equation \ref{eq:value_function} making TD target an estimation of the total expected return.\\

Here's the TD value estimation algorithm with the new update rule combining bootstrapping and direct sampling:

\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.6]{figures/TD_value_estimation.PNG}
  \caption{Temporal Difference value estimation algorithm; Source: \cite{lecture_mfl}}
  \label{fig:td}
\end{figure}

\subsection{Bias-Variance tradeoff for TD and MC}
In statistics the Bias-Variance tradeoff typically refers to 
a compelled compromise between variance and bias, i.e. either high variance and low bias or low variance and high bias. \\

We can see that $R_t = r_{t+1} + \gamma r_{t+2} + \gamma ^2 r_{t+3} + ...$ is an unbiased estimate of $V^{\pi} (s_t)$ by the definition \ref{eq:value_function_def} of the state value function itself. So if the update rule for TD was $r_{t+1} + \gamma V^{\pi}(s_{t+1})$ it would be an unbiased estimate of $V^{\pi}(s_{t})$ (given $V^{\pi}(s_{t+1})$ is the true value function). But what we actually have is an estimate of the $V^{\pi}(s_{t+1})$ — $V$ ($r_{t+1} + \gamma V(s_{t+1})$ in the update rule) and so the expression for TD target is the biased estimate of $V^{\pi}(s_t)$ \cite{lecture_mfl}. \\

Now if we compare TD and MC methods we can see that TD has some bias with MC being generally unbiased, at the same time TD has much lower variance since the estimate is based only on one transition (s, a, r, s'), while MC return estimate depends on many transitions (trace of episodes). The immediate implications of this is that MC methods have good convergence,
if the simulation runs for long enough but TD is usually more efficient, so it requres less time to learn the value function \cite{lecture_mfl}.
% But due to its nature it is more sensitive to the initalization values for value function.

\section{Model Free Control }
Now we know how to estimate the value function with no model knowledge. The next step is to learn to optimize it and learn the optimal policy. 

% We cannot refer to MDP in 2 scenarios: MDP is simply unknown or MDP is well defined but too big to generalise/estimate (Curse of dimensionality).

\subsection{Generalised Policy Iteration on State Value function} 
One approach is Generalised Policy Iteration (similar in effect to the Monte-Carlo policy iteration above in figure \ref{fig:pia}). But we cannot apply the exact Policy improvement as before since the update rule for the policy being improved if you remember is: 
\begin{align}
    \pi(s) \leftarrow arg \max_{a} \sum_{s'} P^{a}_{ss'} [R^{a}_{ss'} + \gamma V(s')]
\end{align}
Here we need the knowledge of the state transition probabilities $P$ and the immediate reward function $R$. This is beyond our knowledge in a Model-free situation. Thus we can choose the policy greedily based on the state-action value function $Q(s, a)$ instead, which fits our Model-free narrative \cite{lecture_mfc}:  
\begin{align}
    \pi(s) \leftarrow arg \max_{a} Q(s, a) 
\end{align}
% "Formally there's no difference between transitions between states and value of states and transitions between state-action pairs and state-action value function respectively. Both can be seen as Markov chains with a reward process."
\subsection{MC Policy improvement theorem}

Similarly to how Policy Improvement Theorem can be proven for Dynamic Programming, it can be shown that $Q^{\pi_k}(s, \pi_{k+1}(s)) \geq V^{\pi_k}(s)$ implies that $\pi_{k+1} \geq \pi_k$ \cite{lecture_mfc}. 
% "Thus each subsequent policy is better than the previous one and this means convergence to optimal policy and value functions." 

\subsection{On and Off policies}

% One of the important prerequisites for the above proof is the 
% «Exploring starts» assumption, 

% i.e. the agent needs to learn the optimal policy by infinitely selecting all possible actions. There are 2 ways to ensure the agent continues to 
% explore all options: on-policy and off-policy methods. \\

\textbf{On-policy} methods use one policy to estimate value functions and choose actions (i.e. generate data), while \textbf{off-policy} methods separate these into 2 policies \cite{lecture_mfc}. 

\subsection{On-policy soft control}

Soft policies have a property such that all actions have a chance to be explored, i.e. $\pi(a, s) > 0 \forall s \in S, \forall a \in A.$\\

A form of soft-policies are $\epsilon-$greedy policies, where 
you can adjust a parameter $\epsilon$ to weigh a particular action such that it has a higher probability of being chosen,
while all the other actions are equally probable (relative to each other) \cite{lecture_mfc}:\\\\
$\epsilon$-greedy policy with $\epsilon \in [0, 1]$:
\begin{align}
    \pi(s, a) = \begin{cases} 
        1 - \epsilon + \frac{\epsilon}{|A(s)|}, \text{ if } a^{*} = arg \max_{a} Q(s, a) \\
        \frac{\epsilon}{|A(s)|}, \text{ if }  a \neq a^*
    \end{cases}   
\end{align}

\subsection{Monte Carlo Control algorithm}

Now adding the policy update to the MC Policy Evaluation algorithm we combine the evaluation and the improvement steps into one loop:

\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/fv_greedy_mc_control.PNG}
  \caption{On-policy $\epsilon$-greedy first-visit MC control algorithm; Source: \cite{lecture_mfc}}
  \label{fig:fv_mc_greedy}
\end{figure}
\newpage
\subsection{Policy Convergence}

"Greedy in the Limit with Infinite Exploration" \cite{lecture_mfc} (GLIE) defines the convergence properties of a given policy under certain conditions. 

% \subsection{batch learning}

% Like before there are 2 MC approaches: batch learning and online learning

% \begin{figure}[h!]
%   \centering
% %   \hspace*{-1cm}  
%   \includegraphics[scale=0.5]{figures/mc_batch_greedy.PNG}
%   \caption{MC Batch Learning to Control; Source: Lecture}
%   \label{fig:mc_batch_greedy}
% \end{figure}
% \begin{figure}[h!]
%   \centering
% %   \hspace*{-1cm}  
%   \includegraphics[scale=0.5]{figures/mc_iterative_greedy.PNG}
%   \caption{MC Iterative Learning to Control; Source: Lecture}
%   \label{fig:mc_iterative_greedy}
% \end{figure}

% \subsection{MC summary}
% one of the issues in MC is sufficient exploration of all available actions. In simulated environments this issues can be tackled with randomness for state-action values at the start of  the learning process but harder to arrange in learning from real experience. \\

% "In on-policy the agent commits to exploration and learns the respective stochastic policy while in off-policy an agent learns a deterministic policy separate from the one followed. Typically off-policy methods rely on some form of importance sampling."

\subsection{Temporal Difference Control -- SARSA}
Temporal Difference has several advantages over MC such as lower variance and online learning so why not combine the $\epsilon$-greedy policy improvement and TD update rule from equation \ref{eq:td_update} with Q(S,A) \cite{lecture_mfc} :
\begin{align}
    Q(S, A) \longleftarrow Q(S, A) + \alpha (r + \gamma Q(S', A') - Q(S, A)) \label{eq:sarsa_update_rule}
\end{align}

\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/sarsa.PNG}
  \caption{SARSA - On-Policy learning TD control; Source: \cite{lecture_mfc}}
  \label{fig:sarsa}
\end{figure}

SARSA converges under the conditions of GLIE sequence of policies $\pi^k(a, s)$ and Robbins-Monroe sequence of step-sizes $\alpha_t$. So $\epsilon$ and $\alpha$ need to be adjusted over time \cite{lecture_mfc}.

% "One issue for SARSA is sparse rewards when the reward 
% propagates down a trace through many episodes slowly. Two ways to deal with it are SARSA-Lambda and Hindsight Experience Replay"

\subsection{Off-policy methods}

If you remember in off-policy we use a different policy $\pi'$ to generate the data while estimating and optimizing $Q^{\pi}$ and $V^{\pi}$ still. $\pi$ is the target policy and $\pi'$ is the  behaviour policy. An important assumption to ensure learning the assumption of coverage: $\pi(a, s) > 0 => \pi'(s, a) > 0$ \cite{lecture_mfc}.\\

% "One of the most important breakthroughs in reinforcement learning was the development of off-policy TD control algorithm known as Q-learning (Watkins, 1989)." [LECTURE]

\subsection{Q learning}
In Q-learning the  next action is chosen using the behaviour policy, but in the update rule we use the alternative greedy successive action based on the target policy: $\max_a Q(s_{t+1}, a)$\\

In this way we allow both the target and behaviour policies to improve, target policy being greedy w.r.t. $Q(S, A)$ and behaviour policy being $\epsilon$-greedy w.r.t $Q(S, A)$. The Q learning target is then respectively: $r_{t+1} + \max_{a'} \gamma Q(s_{t+1}, a')$\\

\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/q_learning.PNG}
  \caption{Q-Learning algorithm; Source: \cite{lecture_mfc}}
  \label{fig:q_learning}
\end{figure}

Note that there are no explicit policies here: target policy is implicit in the greedy term $max_a Q(S', a)$ and behaviour policy is the $\epsilon$-greedy version of the target policy that we use to choose $A$. Both policies are updated with the update rule.

\subsection{Cliff walking  Q-learning vs. SARSA}

\section{Function Approximation}

\subsection{Limitations of tabular}

In Tabular Q learning due to the discrete nature of 
state and action spaces it is harder to represent the real world accurately as the problem of fine discretisation arises: the approximation needs to be accurate enough for meaningful learning. But this can be memory inefficient as the number of parameters grows \cite{lecture_intro_to_deep_rl}. \\

Another important limitation affects the exploration ability of the learning agent: each Q entry for subsequent state-action entries need to be updated independently despite obvious correlation between them.\\

One solution to this is function approximation.

\section{Deep learning}

A neural network consists of multiple layers of interconnected neurons each defined by a weight and bias that are applied to the incoming signal \cite{lecture_intro_to_deep_rl}.\\

In supervised learning there's usually a reference that 
the network is trained against, for example in classification the reference might be a label to an RGB image. The model then learns weights such that the desired outputs are produced on respective inputs \cite{lecture_intro_to_deep_rl} . \\

This is achieved through backpropagation and gradient descent where the difference between current weights and desired weights is evaluated using a cost functions and applied until convergence \cite{lecture_intro_to_deep_rl} .\\

For example for a dataset with $x_i$ input paired with a $y_i$ label cost function would be: 
\begin{align}
    C(\theta, D) = [y_i - f(x_i)]^2
\end{align}

\section{Deep Q learning}

So our goal now is to approximate the Q-table with a neural network that accepts state and outputs Q(s, a) 
values for the whole range of actions \cite{lecture_intro_to_deep_rl}. Thus we make state space continuous, while action space is still discrete, like in this example:

\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/q_network.PNG}
  \caption{Example Q network for traversing a 2D space; Source: \cite{lecture_intro_to_deep_rl}}
  \label{fig:q_network  }
\end{figure}

\subsection{cost function}
Tranferring the TD error from equation \ref{eq:td_update} into the context of neural network cost function we get \cite{lecture_intro_to_deep_rl}:
\begin{align}
    C(\theta, D) = [R + \gamma \max_a Q(S', a) - Q(S, A)]^2
\end{align}
\begin{figure}[h!]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/dqn_first.PNG}
  \caption{Deep Q-learning algorithm; Source: \cite{lecture_dqn}}
  \label{fig:dqn_first}
\end{figure}

\subsection{Overfitting}
% Like any neural network it can overfit but the random starts nature of LR counteracts that in effect [QnA]

\section{Experience Replay Buffer}

One limitation of online learning is that one transition needs to be visited many times for the network to learn appropriate weights ("many updates per transition" \cite{lecture_dqn}). \\

A solution is to create a dataset of all the experienced data so far collected by the agent. The advantage is that any transition can be sampled from the replay buffer and trained on more than once \cite{lecture_dqn}. 

\subsection{Mini batch learning }

Another limitation of deep Q-learning is that since now Q is represented not by a table, but by a continous function, neighbouring state values will affect each other easily due to correlation between them \cite{lecture_dqn} . And if 
we update the function in a consecutive manner (going by consecutive states in the order they were collected for example) this will cause the  Q-distribution to start changing rapidly and affect the untrained parts of the Q-network and cause uneven learning \cite{lecture_dqn}. \\

One solution is to sample transitions from the replay buffer not in the order they were collected but in random mini-batches\cite{lecture_dqn}. This causes a more uniform, even learning and update of the Q function \cite{lecture_dqn}.  

\begin{figure}[H]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/replay_buffer.PNG}
  \caption{Deep Q -learning algorithm with replay buffer; Source:\cite{lecture_dqn}}
  \label{fig:replay_buffer}
\end{figure}

\subsection{Prioritised Experience Replay}

On top of sampling transitions in minibatches in a uniform random manner it may be useful to note that some transitions might be more important than others and sampling them more frequently may boost the learning process. \cite{lecture_dqn}. Hence the idea of prioritised experience replay buffer. \\

A way to tell which transitions are more important than others is to see the magnitude of the prediction error $\delta = R + \gamma \max_a Q_{\theta}(S', a) - Q_{\theta}(S, A)$. 
Intuitively greater error results in greater learning.\\

So each transition is assigned a weight $w_i = |\delta| + \epsilon$, where $\epsilon$ is a small constant ensuring each transition has a probability of being sampled greater than 0. Thus the sampling probability for a transition $i$ is:
\begin{align}
    p_i = \frac{w_i^{\alpha}}{\sum_k w^{\alpha}_k}
\end{align}

Where $\alpha$ determines how much individual transitions are prioritized over the others with $\alpha = 0$ being equivalent to uniform sampling.\\

It only makes sense to update only the weights for transitions that were sampled, since we already have their $\delta$ error values\cite{lecture_dqn}. For new transitions pushed to the replay buffer, assign the current maximum weight to ensure that they can be sampled and that sufficient exploration is done.

\begin{figure}[H]
  \centering
%   \hspace*{-1cm}  
  \includegraphics[scale=0.5]{figures/prioritized_buffer.PNG}
  \caption{Prioritised Experience Replay Algorithm; Source: \cite{lecture_dqn}}
  \label{fig:replay_buffer}
\end{figure}

\section{Target Network }

As we have seen before with mini-batch sampling having an approximated Q function introduces certain challenges. 
For example when adjacent states are trained on multiple times
it results in a rocketing effect with the Q function basically "chasing its own tail" in a sense \cite{lecture_dqn}.  
\begin{figure}[h!h!h!h]
\centering
\includegraphics[scale=0.5]{figures/target 1.PNG}
\includegraphics[scale=0.5]{figures/target 2.PNG}\\
\includegraphics[scale=0.5]{figures/target 3.PNG}
\includegraphics[scale=0.5]{figures/target 4.PNG}
  \caption{Generalisation effect on learning; Source: \cite{lecture_dqn}}
  \label{fig:generalisation_curse}
\end{figure}

This correlation between adjacent states can be tracked down to the $\max_a Q_{\theta}(S', a)$ term in the update step. Hence the 
extension to DQN is to introduce a separate Target Network $\hat{Q}$ to calculate the prediction error. This allows Q network to learn independently from the generalisation effect \cite{lecture_dqn}. Every now and then $\hat{Q}$ is set to Q.

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{figures/target 0.PNG}
  \caption{Target network; Source: \cite{lecture_dqn}}
  \label{fig:target_network}
\end{figure}

Here's the updated algorithm for Deep Q-learning with target network extension.

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{figures/dqn wth target.PNG}
% \caption{The Universe}
  \caption{Deep Q-Learning with Target Network; Source: Lecture}
  \label{fig:target_network_algorithm}
\end{figure}

\section{Double Deep Q-learning}
Another issue that still arises is the problem of overestimation when picking the greedy action in the target $R + \gamma \max_a \hat{Q}_{\hat{\theta}}(S' a)$. Target network is a noisy estimation for different action values (for an arbitraty state) and the sampled maximum $\hat{Q}$ value might not be the maximum for the true $Q$ function for the same action \cite{lecture_dqn}. In order to tackle this problem of overestimation 
further decoupling is introduced in the target term.\\

In Double Deep Q-learning different networks are used to pick the maximum action and estimate its Q value. This on average reduces the amount by which an action is overestimated since an unusually high value in one network is unlikely to be such in the other \cite{lecture_dqn}. Thus the new error in the update step in the algorithm is either:
\begin{align}
    [R + \gamma Q_{\theta}(S', \text{arg}\max_a\ \hat{Q}_{\hat{\theta}}(S', a)) - Q_{\theta}(S, A)]^2
\end{align}
or:
\begin{align}
    [R + \gamma \hat{Q}_{\hat{\theta}}(S', \text{arg}\max_a\ Q_{\theta}(S', a)) - Q_{\theta}(S, A)]^2
\end{align}


% \subsection{3 network architectures}
% \subsection{REINFORCE ALGORITHM}
% \subsection{deterministic vs stsochastic policies}
% \subsection{reducing variance}
% \subsection{model free vs model based}
% \subsection{planning in discrete spaces (irrelevant to LoRa) }
% \subsection{Cross Entropy }
% \subsection{method for continuous spaces (relevant?)}
% \subsection{model predictive control}

\chapter{Learning Model}
\section{Distributed Learning}

The ideal situation would of course be having one learning agent 
per LoRa node so they could learn a policy specifically suited for
that one node. But the case is that typically nodes located 
near each other can benefit from one policy [CITATION or EXPERIMENT]
so in order not to waste memory on slightly different models 
for nodes located near each other nodes can be grouped into clusters
and share learning weights.



Despite first prototypes starting with 

\section{LoRa State}
\section{LoRa Action}
\section{LoRa Reward}

